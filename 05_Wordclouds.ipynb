{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word clouds\n",
    "\n",
    "To compare the word occurrences in the three papers and visualise them using word clouds, we need to follow these steps:\n",
    "\n",
    "* Filter out stopwords.\n",
    "* Generate word frequencies.\n",
    "* Create word clouds for each `krantnaam`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "Import the necessary packages for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialize\n",
    "with open('data/preprocessed_docs.pkl', 'rb') as f:\n",
    "    processed_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "Then we'll import our Dutch NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the relative path to the model directory\n",
    "model_path = \"model/nl_core_news_sm\"\n",
    "\n",
    "# Load the model from the relative path\n",
    "nlp = spacy.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Stop words are a set of commonly used words in a language. Examples of stop words in English are “a,” “the,” “is,” “are,” etc. Stop words are commonly used in Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information.\n",
    "\n",
    "We will store stopwords in a Python `set` data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the Dutch stopwords from the model\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "# print first 10 stopwords\n",
    "list(stopwords)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions to get word frequencies\n",
    "\n",
    "The function below helps us extract word frequencies and store them in one `dictionary` data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lemmas(lemmas, filter_sets):\n",
    "    # Combine all filter sets into one set\n",
    "    combined_filters = set().union(*filter_sets)\n",
    "    return [lemma for lemma in lemmas if lemma.lower() not in combined_filters and lemma.isalpha()]\n",
    "\n",
    "def create_word_frequencies(processed_docs, filter_sets):\n",
    "    word_frequencies = {}\n",
    "    \n",
    "    for krantnaam in processed_docs['krantnaam'].unique():\n",
    "        # Filter DataFrame for the current krantnaam\n",
    "        df_filtered = processed_docs[processed_docs['krantnaam'] == krantnaam]\n",
    "        \n",
    "        # Get all lemmas for the current krantnaam, filtering out stopwords and common short words\n",
    "        all_lemmas = []\n",
    "        for lemmas in df_filtered['lemmas']:\n",
    "            all_lemmas.extend(filter_lemmas(lemmas, filter_sets))\n",
    "        \n",
    "        # Calculate word frequencies\n",
    "        word_freq = Counter(all_lemmas)\n",
    "        \n",
    "        # Store the word frequencies in the dictionary\n",
    "        word_frequencies[krantnaam] = word_freq\n",
    "        \n",
    "    return word_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word use\n",
    "\n",
    "Let's have a look which words are most frequently used in each paper.\n",
    "\n",
    "* 'De standaard'\n",
    "* 'Het vaderland : staat- en letterkundig nieuwsblad'\n",
    "* 'De Tĳd : godsdienstig-staatkundig dagblad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate word frequencies without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = create_word_frequencies(processed_docs, filter_sets = [stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(word_frequencies['De standaard']).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, OCR rubbish..., let's do a bit more filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a filter set for OCR rubbish\n",
    "\n",
    "A lot of short words appear to be OCR rubbish.\n",
    "\n",
    "Let's find all short lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find lemmas with less than X characters\n",
    "all_lemmas = [lemma for lemmas in processed_docs['lemmas'] for lemma in lemmas]\n",
    "\n",
    "# Identify common short lemmas (less than X characters)\n",
    "short_words = set(lemma for lemma in all_lemmas if len(lemma) < 5)\n",
    "\n",
    "# print first 10 short_words\n",
    "list(short_words)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate word frequencies without stopwords, short_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = create_word_frequencies(processed_docs, filter_sets = [stopwords, short_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(word_frequencies['De standaard']).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate word frequencies without stopwords, short_words and your own filter?\n",
    "\n",
    "Make sure to put your words in the set in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter_set = {'jan','alhier','maken','komen','pct'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = create_word_frequencies(processed_docs, filter_sets = [stopwords, short_words, my_filter_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(word_frequencies['De standaard']).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clouds\n",
    "\n",
    "The code creates threw word clouds, do not forget to adjust the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter_set = {'jan','alhier','maken','komen','pct','aand','april','julij'}\n",
    "\n",
    "word_frequencies = create_word_frequencies(processed_docs, filter_sets = [stopwords, short_words, my_filter_set])\n",
    "\n",
    "# Function to generate word cloud\n",
    "def generate_wordcloud(word_freq, krantnaam, max_words=50):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=max_words).generate_from_frequencies(word_freq)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Word Cloud for {krantnaam}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate word clouds for each krantnaam\n",
    "for krantnaam, word_freq in word_frequencies.items():\n",
    "    generate_wordcloud(word_freq, krantnaam, max_words=50)  # Adjust max_words as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions?\n",
    "\n",
    "Now that we have these cloud what can we interpret from them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
