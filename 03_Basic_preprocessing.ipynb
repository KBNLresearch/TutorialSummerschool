{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic preprocessing of the corpus\n",
    "\n",
    "In this Notebook, we will some basic preprocessing of the corpus. This is necessary to analyze and visualize the corpus in the upcoming Notebooks in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run notebook 02\n",
    "\n",
    "We need this so that we can use the variables from that notebook. You can ignore the outputs from these cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 02_Loading_data_and_visualizing_corpus.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if DataFrame is stored correctly\n",
    "\n",
    "Let's quickly check if the DataFrame is stored correctly. We do this by displaying the first 5 rows of the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Now we can move on to the actual preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Natural Language Processing (NLP) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SpaCy](https://spacy.io/) offers language models which you can import and use to perform natural language processing. We load the library and the appropriate Dutch model for our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.nl.examples import sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the small Dutch natural language processing (NLP) package. This package can do:\n",
    "* Tokenization:\n",
    "  Breaking down text into individual tokens (words, punctuation marks, etc.).\n",
    "* Part-of-Speech (POS) Tagging:\n",
    "  Assigning grammatical categories (such as nouns, verbs, adjectives) to each token.\n",
    "* Named Entity Recognition (NER):\n",
    "  Identifying and classifying named entities (like people, organizations, locations) in the text.\n",
    "* Dependency Parsing:\n",
    "  Analyzing the grammatical structure of a sentence, establishing relationships between tokens.\n",
    "* Lemmatization:\n",
    "  Reducing words to their base or dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Specify the relative path to the model directory\n",
    "model_path = \"model/nl_core_news_sm\"\n",
    "\n",
    "# Load the model from the relative path\n",
    "nlp = spacy.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SpaCy Doc objects\n",
    "\n",
    "When you call `nlp` on a text, SpaCy first tokenizes the text to produce a Doc object.\n",
    "\n",
    "We will add Doc objects to our dataframe.\n",
    "\n",
    "A [Doc object](https://spacy.io/api/doc/) is container for accessing linguistic annotations.\n",
    "\n",
    "Create a helper function. We will use this in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column in the pandas DataFrame, called \"doc\".\n",
    "\n",
    "This column will store the content of each article in a way that is easy to use in later steps of this tutorial. \n",
    "\n",
    "In order to make the Doc objects, we first need to get rid of the rows in the dataframe that do not contain any content of the article. This is represented by a 'NaN' in the 'content' column. We can use the 'dropna' function to get rid of the NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add them by executing this Python command, but it takes a long time, depending on your computer's memory.\n",
    "\n",
    "```data[\"doc\"] = data[\"content\"].apply(process_text)```\n",
    "\n",
    "Let's assume this has been done. Open the processed data with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Deserialize\n",
    "with open('data/processed_docs.pkl', 'rb') as f:\n",
    "    processed_docs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization refers to the process of breaking down a piece of text into small units, called 'tokens'. In our case, the tokens are the words in an article, but tokens can also consist of parts of words or characters. Tokenization is a crucial part of NLP.\n",
    "\n",
    "Create a helper function. We will use this in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(doc):\n",
    "    return [(token.text) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"Token\" column in the DataFrame. This column stores the words in each article as a list, which is useful in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs['tokens'] = processed_docs['doc'].apply(get_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the process of reducing words to their most basic form, known as the lemma. For example, the lemma of 'running' is 'run' and the lemma of 'better' is 'good'. Lemmatization is important for NLP, because it reduces the complexity of a text, improves accuracy of many NLP tasks, and leads to better semantic understanding. \n",
    "\n",
    "Create a helper function. We will use this function in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(doc):\n",
    "    return [(token.lemma_) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 'lemma' column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs['lemmas'] = processed_docs['doc'].apply(get_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display lemmas and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[['tokens', 'lemmas']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the tokens with the lemmas, we can see how some words have been reduced to their root form. For example, 'viel' has been changed to 'vallen'. However, we can also see that 'tweeden' has been changed to 'tweed'. This change is more questionable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy features an extremely fast statistical entity recognition system. The default trained pipelines can identify a variety of named and numeric entities, including companies, locations, organizations and products.\n",
    "\n",
    "Named entities are available as the `ents` property of a `Doc` object.\n",
    "\n",
    "The function `spacy.explain` will return a description for a given entity type (tag).\n",
    "\n",
    "Try it out and see if you can find the meaning of this entities\n",
    "* FAC\n",
    "* PERSON\n",
    "* NORP\n",
    "* GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('FAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Let's add some GPE-entities to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpe(doc):\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == 'GPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs['GPEs'] = processed_docs['doc'].apply(get_gpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first few rows of GPEs as a list\n",
    "gpe_list = processed_docs[['identifier','GPEs']].head(2).values.tolist()\n",
    "for item in gpe_list:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the preprocessed dataset into separate file\n",
    "\n",
    "We will create a new file, called 'data_preprocessed.csv'. We will use this in the next Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs.to_csv('data/data_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case: count the number of words in each article\n",
    "\n",
    "In the next code block, we will count the number of words in each article and store it in a separate column, called 'article_length'. We will use this information to make a comparison between the different newspapers in the next Notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve the length of each article in the corpus and store it in the DataFrame\n",
    "\n",
    "## Create empty list to store the lengths\n",
    "article_lengths = []\n",
    "\n",
    "## Retrieve length of each article and store in list\n",
    "for index, row in processed_docs.iterrows():\n",
    "    article_lengths.append(len(row['tokens']))\n",
    "\n",
    "## Append list to DataFrame\n",
    "processed_docs['article_length'] = article_lengths\n",
    "\n",
    "## Show the first rows of title, tokens and article length in DataFrame                                    \n",
    "processed_docs[['title', 'tokens', 'article_length']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
